{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Probability\n",
    "概率论难在计算，尤其是条件概率相关的一些计算，这篇笔记主要用来记公式以及一些计算技巧。  \n",
    "\n",
    "概率论计算的一些思路：\n",
    "1. 计算什么？概率/期望/方差\n",
    "2. 离散还是连续\n",
    "3. 如何设置事件\n",
    "4. 推导思路\n",
    "  \n",
    "条件概率难点和重要性：\n",
    "1. 当我们无法直接研究事件A的概率/期望/分布的时候，我们通常借助事件B来对其进行研究。\n",
    "2. 通过事件B来研究A，其实就是在研究A和B的联合分布，但是联合分布现实中是很难求得的，所以借助条件概率。\n",
    "3. 条件概率同时还能用于研究两个事件的依赖关系。\n",
    "\n",
    "\n",
    "条件概率和机器学习：\n",
    "1. 条件概率-贝叶斯公式-极大似然估计/贝叶斯估计 是ML中朴素贝叶斯法的思考路线。  \n",
    "2. 条件概率-经验熵/互信息-依据熵增益剪枝 是ML中决策树的思考路线。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 条件概率及贝叶斯三种形式\n",
    ">条件概率基本形式：$$P(A|B)=\\frac{P(A\\bigcap B)}{P(B)}$$  \n",
    "\n",
    ">贝叶斯基本形式：$$P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$$  \n",
    "- 这个公式在技巧性很强，在很多基础证明推导中求$P(A|B)$都可以转换成求$P(B|A)$问题\n",
    "- 我个人喜欢称之为条件概率的换底公式\n",
    "\n",
    ">全概率下贝叶斯公式：$$P(A_k|B)=\\frac{P(B|A_k)P(A_k)}{\\sum_{i=1}^{n}P(B|A_i)P(A_i)}$$\n",
    "- 贝叶斯方法的思想，当我们知道B信息想求A的时候(即想计算$P(A|B)$)，我们可以把$P(B|A)$当作学习模型(即知道结果A反推信息B)\n",
    "- $A_i$是完备事件，比如在手写数字识别中，我们可以设$A_0$到$A_9$代表数字0-9\n",
    "\n",
    ">贝叶斯延伸公式：$$P(A|B)=P(C|B) \\cdot P(A|C)+P(\\bar{C}|B) \\cdot P(A|\\bar{C})$$\n",
    "- 当我们发现B条件下A的概率不容易求的时候，我们可以加入条件C试试。\n",
    "- 这个公式可以无限延伸到其他事件（需要是完备的）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 条件概率和联合分布\n",
    "### 公式关系图\n",
    "![](./FormulaRelation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tower property*\n",
    "也叫[Law of total expectation](https://en.wikipedia.org/wiki/Law_of_total_expectation)(Wikipedia)  \n",
    "$$E[A]=E[E(A|B)]$$  \n",
    "这个公式比较难理解的一点是，$E(A|B)$虽然是期望但同时也是一个随机变量，我个人的理解是它其实内部形成了一个映射B$\\rightarrow$A,即形成了一个A随B变化下的随机变量分布，再对这个求期望就是X的期望了。\n",
    "\n",
    "假设A是个0-1事件，用一个Indicator function将其与事件X对应：\n",
    "$$ A=\\left\\{\n",
    "\\begin{aligned}\n",
    "& 1, & if \\ X \\ occurs \\\\\n",
    "& 0, & if \\ X \\ not occurs\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "则有$E(A)=P(X)$，分别对其作用条件Y则有$E(A|Y)=P(X|Y)$,\n",
    "推导出全概率公式:\n",
    "$$\n",
    "\\begin{align}\n",
    "E[A] &=E[E(A|Y)]\\\\\n",
    "     &=\\sum \\limits_{y} E(A|Y=y)P(Y=y)\\\\\n",
    "     &=\\sum \\limits_{y}P(X|Y=y)P(Y=y)\\\\\n",
    "     &=P(X)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "全概率公式[Law of total probability](https://en.wikipedia.org/wiki/Law_of_total_probability)(Wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent \n",
    "如果事件X,Y独立：\n",
    "> **分布函数&密度函数:**\n",
    "1. 定义$P(X\\cap Y)=P(X)\\cdot P(Y)$\n",
    "2. $P(X^C \\cap Y)=P(X^C)\\cdot P(Y)$\n",
    "3. $P(X \\in A,Y \\in B)=P(X \\in A) \\cdot P(Y \\in B)$\n",
    "4. 联合分布-边际分布:$p(x,y)=p_X(x) \\cdot p_Y(y)$  or $f(x,y)=f_X(x) \\cdot f_Y(y)$\n",
    "5. $F(a,b)=F_X(a) \\cdot F_Y(b)$\n",
    "\n",
    "> **期望&方差:**\n",
    "1. $E[XY]=E[X]\\cdot E[Y]$\n",
    "2. $E[g(X)h(Y)]=E[g(X)]\\cdot E[h(Y)]$\n",
    "3. $Var(X+Y)=Var(X)+Var(Y)$\n",
    "\n",
    "> **Conditional:**\n",
    "1. $P(X|Y)=P(X)$\n",
    "2. $E[X|Y]=E[X]$\n",
    "3. $Var(X|Y)=Var(X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 技巧\n",
    "### Trick 1\n",
    "Indicator function：\n",
    "对事件E，定义一个indicator function\n",
    "$$ I=\\left\\{\n",
    "\\begin{aligned}\n",
    "& 1, & if \\ E \\ occurs \\\\\n",
    "& 0, & if \\ E \\ not occurs\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "尤其是当定义的事件独立时，Indicator function很好用。\n",
    "### Trick 2\n",
    "设置某个事件用全概率公式。  \n",
    "离散：$P(X)=\\sum \\limits_{y}P(X|Y=y)P(Y=y)$  \n",
    "连续：$P(X)=\\int_{-\\infty}^{+\\infty}P(X|Y=y)P(Y=y)dy$\n",
    "再将$P(X|Y=y)$转化成等价事件/概率。\n",
    "有时设置的条件事件y只有两种即Y=0和Y=1，简化运算。\n",
    "### Trick 3\n",
    "设置其他事件做等价转换：\n",
    "1. 巧用事件独立性。\n",
    "2. 巧用事件无记忆性。\n",
    "3. 巧用事件之间的迭代，然后算数列的和。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 浅谈条件概率和机器学习\n",
    "### 朴素贝叶斯法\n",
    "- 朴素贝叶斯：其实就是把先验概率$P(X=x|Y=C_k)$作为模型，并用上面说的换底公式后计算$\\arg \\max\\limits_{c_k} \\ P(Y=C_k|X=x)$，其中由于贝叶斯全概率公式分母部分想同，只要看分子部分即可。\n",
    "- 模型的学习方法：其实就是对朴素贝叶斯公式中的$C_k$进行参数估计，常用方法是极大似然估计，考虑到先验条件概率会有=0的情况进行了贝叶斯估计进行完善。  \n",
    "- 为何“朴素”：朴素贝叶斯“朴素”的原因是因为进行了条件独立假设，该假设意味着在先验概率条件下，不同输入特征是相互独立的。\n",
    "\n",
    "### 决策树\n",
    "- 决策树的难点是增枝和剪枝，其基础是信息论中，常用的有信息增益（ID3）和信息增益比（C4.5）。\n",
    "- 信息增益=经验熵-经验条件熵，条件熵的基础是条件概率。\n",
    "- 当然决策树的生成可能也不和条件概率相关，还有其他生成决策树的方法比如即Gini、最小二乘等等。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "207px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
